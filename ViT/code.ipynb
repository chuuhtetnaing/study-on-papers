{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbde1e1-1173-4669-9a1d-45f2507f1a8d",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b80bf-610d-4041-a519-d90fb999ee0b",
   "metadata": {},
   "source": [
    "# 1. Patch Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8129bf8c-cfcb-4e9d-b257-536df2e21429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T09:17:09.231996Z",
     "start_time": "2023-08-13T09:17:09.228195Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-13T06:56:22.245697Z",
     "iopub.status.busy": "2023-08-13T06:56:22.244907Z",
     "iopub.status.idle": "2023-08-13T06:56:22.251860Z",
     "shell.execute_reply": "2023-08-13T06:56:22.251142Z",
     "shell.execute_reply.started": "2023-08-13T06:56:22.245665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is square).\n",
    "        \n",
    "    patch_size : int\n",
    "        Size of the patch (it is square).\n",
    "    \n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "        \n",
    "    embed_dim: int\n",
    "        The embedding dimension.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "         \n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches, embed_dim)`.\n",
    "        \"\"\"\n",
    "    \n",
    "        x = self.proj(x)     # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2)     # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1,2) # (n_samples, n_patches, embed_dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741af32d-20d7-459c-9f45-9f62d6362868",
   "metadata": {},
   "source": [
    "## 1.1 Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15951fe-cb58-405e-82bd-c71de540b48c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T09:45:13.700517Z",
     "start_time": "2023-08-13T09:45:13.698579Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-13T07:19:14.712993Z",
     "iopub.status.busy": "2023-08-13T07:19:14.712194Z",
     "iopub.status.idle": "2023-08-13T07:19:14.720369Z",
     "shell.execute_reply": "2023-08-13T07:19:14.719678Z",
     "shell.execute_reply.started": "2023-08-13T07:19:14.712967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape     : torch.Size([1, 3, 224, 224])\n",
      "Output Conv2d Shape   : torch.Size([1, 100, 14, 14])\n",
      "Processed Output Shape: torch.Size([1, 196, 100])\n"
     ]
    }
   ],
   "source": [
    "conv_kwargs = {\n",
    "    \"in_channels\": 3,\n",
    "    \"out_channels\": 100,\n",
    "    \"kernel_size\": 16,\n",
    "    \"stride\": 16\n",
    "}\n",
    "\n",
    "proj_layer = nn.Conv2d(**conv_kwargs)\n",
    "\n",
    "img_kwargs = {\n",
    "    \"n_samples\": 1,\n",
    "    \"channel\": 3,\n",
    "    \"img_size_1\": 224,\n",
    "    \"img_size_2\": 224,\n",
    "}\n",
    "\n",
    "input_img = torch.randn(*img_kwargs.values())\n",
    "embed_img = proj_layer(input_img)\n",
    "\n",
    "processed_output_img = embed_img.flatten(2)\n",
    "processed_output_img = processed_output_img.transpose(1,2)\n",
    "\n",
    "\n",
    "print(\"Input Image Shape     :\", input_img.shape)\n",
    "print(\"Output Conv2d Shape   :\", embed_img.shape)\n",
    "print(\"Processed Output Shape:\", processed_output_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194b984-694d-4da0-bacb-b11ba6fbec19",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Output shape is 14 because the kernel_size and stride are 16.\n",
    "    There are 16 patches in rows to cover 224 width (224/16 = 14).\n",
    "    Similarly, there are 16 patches in columns to cover 224 height (224/16 = 14).\n",
    "</p>\n",
    "<p>\n",
    "    Total number of patches are 196 (224/16 ** 2) where \n",
    "    each row has 14 patches and each column has 14 patches\n",
    "    which result in 14 ** 2 (224/16 ** 2.\n",
    "</p>\n",
    "<p>\n",
    "    Finally, we'll flatten (14, 14) patches into a vector to get a flatten output.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b790de-a417-45c4-89e5-1a9e7cdc1972",
   "metadata": {},
   "source": [
    "## 1.2 Demo on Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7482473-d3ca-48e5-bd56-5573f96bd70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T09:45:15.339504Z",
     "start_time": "2023-08-13T09:45:15.324907Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-13T07:17:44.648573Z",
     "iopub.status.busy": "2023-08-13T07:17:44.647656Z",
     "iopub.status.idle": "2023-08-13T07:17:44.655834Z",
     "shell.execute_reply": "2023-08-13T07:17:44.655039Z",
     "shell.execute_reply.started": "2023-08-13T07:17:44.648542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape  : torch.Size([1, 3, 224, 224])\n",
      "Output Conv2d Shape: torch.Size([1, 196, 100])\n"
     ]
    }
   ],
   "source": [
    "patch_kwargs = {\n",
    "    \"img_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"in_chans\": 3,\n",
    "    \"embed_dim\": 100,\n",
    "}\n",
    "patch_embedding = PatchEmbed(**patch_kwargs)\n",
    "input_img = torch.randn(*img_kwargs.values())\n",
    "embed_img = patch_embedding(input_img)\n",
    "\n",
    "print(\"Input Image Shape  :\", input_img.shape)\n",
    "print(\"Output Conv2d Shape:\", embed_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3989b-a3d1-496f-ac07-a2a91629d2e3",
   "metadata": {},
   "source": [
    "# 2. Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3feac77-ecd6-4edf-93b8-bb59d6d0b100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T09:50:50.393974Z",
     "start_time": "2023-08-13T09:50:50.393316Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention Mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and output dimension of per token features.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\\\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing constant for the dot product.\n",
    "\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space\n",
    "\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = math.sqrt(1/self.head_dim)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # (n_sample, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.reshape(\n",
    "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        qkv = qkv.permute(\n",
    "            2, 0, 3, 1, 4\n",
    "        )\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dp = (q @ k_t) * self.scale\n",
    "        attn = dp.softmax(dim=-1)\n",
    "\n",
    "        # (n_samples, n_heads, n_patches + 1, head_dim\n",
    "        weighted_avg = attn @ v\n",
    "\n",
    "        # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "\n",
    "        # (n_samples, n_patches + 1, dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87811fc8068a81",
   "metadata": {},
   "source": [
    "## 2.1 Demo on Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e679c86c-bf31-4e2f-9ac3-d40acddeb8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T09:52:27.095100Z",
     "start_time": "2023-08-13T09:52:27.049019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Patch Embedding Shape: torch.Size([1, 196, 100])\n",
      "Output Attention Shape     : torch.Size([1, 196, 100])\n"
     ]
    }
   ],
   "source": [
    "attn_kwargs = {\n",
    "    \"dim\": 100,\n",
    "    \"n_heads\": 10,\n",
    "}\n",
    "\n",
    "attention = Attention(**attn_kwargs)\n",
    "attention_img = attention(embed_img)\n",
    "\n",
    "print(\"Input Patch Embedding Shape:\", embed_img.shape)\n",
    "print(\"Output Attention Shape     :\", attention_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17714f9707b969a0",
   "metadata": {},
   "source": [
    "# 3. MLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45ea0815-14a2-4b97-82ed-b6f747cb7540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T10:07:48.267466Z",
     "start_time": "2023-08-13T10:07:48.142518Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer Perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "\n",
    "    hidden_features : int\n",
    "        Number of nodes in hidden layer.\n",
    "\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The first linear layer.\n",
    "\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "\n",
    "    fc2: nn.Linear\n",
    "        The second linear layer.\n",
    "\n",
    "    drop : nn.Dropout\n",
    "        Dropout Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, out_features)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # (n_samples, n_patches + 1, out_features)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f83e51dd94992",
   "metadata": {},
   "source": [
    "## 3.1 Demo on MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43b015a0-6a79-4774-a6b8-17bec950dc0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T10:07:49.069738Z",
     "start_time": "2023-08-13T10:07:49.052880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Attention Shape: torch.Size([1, 196, 100])\n",
      "Output MLP Shape     : torch.Size([1, 196, 200])\n"
     ]
    }
   ],
   "source": [
    "mlp_kwargs = {\n",
    "    \"in_features\": 100,\n",
    "    \"hidden_features\": 150,\n",
    "    \"out_features\": 200\n",
    "}\n",
    "\n",
    "mlp = MLP(**mlp_kwargs)\n",
    "mlp_out_img = mlp(attention_img)\n",
    "\n",
    "print(\"Input Attention Shape:\", attention_img.shape)\n",
    "print(\"Output MLP Shape     :\", mlp_out_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb15dccf31997a",
   "metadata": {},
   "source": [
    "# 4. Block Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cff2ebed31bc7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T11:31:51.528378Z",
     "start_time": "2023-08-13T11:31:51.526288Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=hidden_features,\n",
    "            out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135f8222f6a8ea3",
   "metadata": {},
   "source": [
    "# 5. Complete VisionTransformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dea16224d11ec963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:12:39.976003Z",
     "start_time": "2023-08-13T12:12:39.953075Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision Transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Both height and width of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Both height and width of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "\n",
    "    depth : int\n",
    "        Number of blocks.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP` module.\n",
    "\n",
    "    qkv_bias : True\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=384,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            n_classes=1000,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            n_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(n_samples, n_classes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # (n_samples, 1, embed_dim)\n",
    "        cls_token = self.cls_token.expand(\n",
    "            n_samples, -1, -1\n",
    "        )\n",
    "\n",
    "        # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0] # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060b8070e0fb8a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4985e3311eaa645a",
   "metadata": {},
   "source": [
    "# 6. Verify Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5d9cc05048753ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:28:44.406030Z",
     "start_time": "2023-08-13T12:28:44.394821Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6ecce3ca6ca34",
   "metadata": {},
   "source": [
    "## 6.1 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4081e8147742e388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:28:45.052860Z",
     "start_time": "2023-08-13T12:28:45.009841Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_n_params(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd629c0d1e66ea38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:22.988752Z",
     "start_time": "2023-08-13T12:51:22.988487Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def assert_tensors_equal(t1, t2):\n",
    "    a1, a2 = t1.detach().numpy(), t2.detach().numpy()\n",
    "    return np.testing.assert_allclose(a1, a2, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b877b5bb856e2ca",
   "metadata": {},
   "source": [
    "## 6.2 Load the Model from Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd11cd544daefbc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:13.609554Z",
     "start_time": "2023-08-13T12:51:12.270587Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"vit_base_patch16_384\"\n",
    "model_official = timm.create_model(model_name, pretrained=True)\n",
    "model_official.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b7ffe8c6bd1c5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:13.939549Z",
     "start_time": "2023-08-13T12:51:13.609163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_config = {\n",
    "    \"img_size\": 384,\n",
    "    \"in_chans\": 3,\n",
    "    \"patch_size\": 16,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"qkv_bias\": True,\n",
    "    \"mlp_ratio\": 4,\n",
    "}\n",
    "\n",
    "model_custom = VisionTransformer(**custom_config)\n",
    "model_custom.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "879d614199111f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:13.949778Z",
     "start_time": "2023-08-13T12:51:13.943462Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token | cls_token\n",
      "pos_embed | pos_embed\n",
      "patch_embed.proj.weight | patch_embed.proj.weight\n",
      "patch_embed.proj.bias | patch_embed.proj.bias\n",
      "blocks.0.norm1.weight | blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias | blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight | blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias | blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight | blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias | blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight | blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias | blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight | blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias | blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight | blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias | blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight | blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias | blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight | blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias | blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight | blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias | blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight | blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias | blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight | blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias | blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight | blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias | blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight | blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias | blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight | blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias | blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight | blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias | blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight | blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias | blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight | blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias | blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight | blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias | blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight | blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias | blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight | blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias | blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight | blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias | blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight | blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias | blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight | blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias | blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight | blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias | blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight | blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias | blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight | blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias | blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight | blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias | blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight | blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias | blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight | blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias | blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight | blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias | blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight | blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias | blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight | blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias | blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight | blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias | blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight | blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias | blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight | blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias | blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight | blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias | blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight | blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias | blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight | blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias | blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight | blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias | blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight | blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias | blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight | blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias | blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight | blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias | blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight | blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias | blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight | blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias | blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight | blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias | blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight | blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias | blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight | blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias | blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight | blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias | blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight | blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias | blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight | blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias | blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight | blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias | blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight | blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias | blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight | blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias | blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight | blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias | blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight | blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias | blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight | blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias | blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight | blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias | blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight | blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias | blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight | blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias | blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight | blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias | blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight | blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias | blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight | blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias | blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight | blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias | blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight | blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias | blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight | blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias | blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight | blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias | blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight | blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias | blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight | blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias | blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight | blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias | blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight | blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias | blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight | blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias | blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight | blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias | blocks.11.mlp.fc2.bias\n",
      "norm.weight | norm.weight\n",
      "norm.bias | norm.bias\n",
      "head.weight | head.weight\n",
      "head.bias | head.bias\n"
     ]
    }
   ],
   "source": [
    "for (n_o, p_o), (n_c, p_c) in zip(\n",
    "    model_official.named_parameters(), model_custom.named_parameters()\n",
    "):\n",
    "    assert p_o.numel() == p_c.numel()\n",
    "    print(f\"{n_o} | {n_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b55e4419cf187",
   "metadata": {},
   "source": [
    "## 6.3 Load the state_dict from Timm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f140d5a0206b44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:35.069878Z",
     "start_time": "2023-08-13T12:51:35.069043Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_custom.load_state_dict(model_official.state_dict())\n",
    "model_custom.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d134d66b01698",
   "metadata": {},
   "source": [
    "## 6.4 Compare the two model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f995516d31a2a96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:36.880474Z",
     "start_time": "2023-08-13T12:51:35.989510Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inp = torch.rand(1, 3, 384, 384)\n",
    "res_c = model_custom(inp)\n",
    "res_o = model_official(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "954144a5a41006b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T12:51:36.890582Z",
     "start_time": "2023-08-13T12:51:36.883336Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_n_params(model_custom) == get_n_params(model_official)\n",
    "assert_tensors_equal(res_c, res_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ef0cf0fc1490d",
   "metadata": {},
   "source": [
    "# 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a89589dd76ede84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T13:00:32.403923Z",
     "start_time": "2023-08-13T13:00:32.402936Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52b115c08f934dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T13:01:08.912415Z",
     "start_time": "2023-08-13T13:01:08.889869Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "imagenet_labels = dict(enumerate(open(\"./inference/classes.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8a07c4b87780a436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T13:04:16.491531Z",
     "start_time": "2023-08-13T13:04:15.811221Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img = (np.array(Image.open(\"./inference/cat.png\")) / 128) -1\n",
    "inp = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).to(torch.float32)\n",
    "logits = model_custom(inp)\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bda6ac69862f419c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T13:05:19.483796Z",
     "start_time": "2023-08-13T13:05:19.482227Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "top_probs, top_ixs = probs[0].topk(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dcd57422046b5803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T13:06:16.885580Z",
     "start_time": "2023-08-13T13:06:16.846716Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tabby, tabby_cat                              --- 0.8001\n",
      "1: tiger_cat                                     --- 0.1752\n",
      "2: Egyptian_cat                                  --- 0.0172\n",
      "3: lynx, catamount                               --- 0.0018\n",
      "4: Persian_cat                                   --- 0.0011\n",
      "5: Siamese_cat, Siamese                          --- 0.0002\n",
      "6: bow_tie, bow-tie, bowtie                      --- 0.0002\n",
      "7: weasel                                        --- 0.0001\n",
      "8: lens_cap, lens_cover                          --- 0.0001\n",
      "9: remote_control, remote                        --- 0.0001\n"
     ]
    }
   ],
   "source": [
    "for i, (ix_, prob_) in enumerate(zip(top_ixs, top_probs)):\n",
    "    ix = ix_.item()\n",
    "    prob = prob_.item()\n",
    "    cls = imagenet_labels[ix].strip()\n",
    "    print(f\"{i}: {cls:<45} --- {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b66e82674e5f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
